{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D analysis\n",
    "\n",
    "This tutorial does a 3D map based analsis on the galactic center, using simulated observations from the CTA-1DC. We will use the high level interface for the data reduction, and then do a detailed modelling. This will be done in two different ways:\n",
    "- stacking all the maps together and fitting the stacked maps\n",
    "- handling all the observations separately and doing a joint fitting on all the maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from pathlib import Path\n",
    "from regions import CircleSkyRegion\n",
    "from gammapy.analysis import Analysis, AnalysisConfig\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    SkyModels,\n",
    "    SkyDiffuseCube,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    PointSpatialModel,\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.spectrum import FluxPointsEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we select observations and define the analysis geometries, irrespective of  joint/stacked analysis. For configuration of the analysis, we will programatically build a config file from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AnalysisConfig()\n",
    "# The config file is now empty, with only a few defaults specified.\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the observations\n",
    "config.observations.datastore = \"$GAMMAPY_DATA/cta-1dc/index/gps/\"\n",
    "config.observations.obs_ids = [110380, 111140, 111159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a reference geometry for the reduced datasets\n",
    "\n",
    "config.datasets.type = \"3d\"  # Analysis type is 3D\n",
    "\n",
    "config.datasets.geom.wcs.skydir = {\n",
    "    \"lon\": \"0 deg\",\n",
    "    \"lat\": \"0 deg\",\n",
    "    \"frame\": \"galactic\",\n",
    "}  # The WCS geometry - centered on the galactic center\n",
    "config.datasets.geom.wcs.fov = {\"width\": \"10 deg\", \"height\": \"8 deg\"}\n",
    "config.datasets.geom.wcs.binsize = \"0.02 deg\"\n",
    "\n",
    "# The FoV offset cut\n",
    "config.datasets.geom.selection.offset_max = 4.0 * u.deg\n",
    "\n",
    "# We now fix the energy axis for the counts map - (the reconstructed energy binning)\n",
    "config.datasets.geom.axes.energy.min = \"0.1 TeV\"\n",
    "config.datasets.geom.axes.energy.max = \"10 TeV\"\n",
    "config.datasets.geom.axes.energy.nbins = 10\n",
    "\n",
    "# We now fix the energy axis for the IRF maps (exposure, etc) - (the true enery binning)\n",
    "config.datasets.geom.axes.energy_true.min = \"0.02 TeV\"\n",
    "config.datasets.geom.axes.energy_true.max = \"20 TeV\"\n",
    "config.datasets.geom.axes.energy_true.nbins = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create separate configs for stacked and joint analysis\n",
    "This is done just by specfiying the flag on `config.datasets.stack`. Since the internal machinery will work differently for the two cases, we will write it as two config files and save it to disc in YAML format for future reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_stack = config.copy(deep=True)\n",
    "config_stack.datasets.stack = True\n",
    "\n",
    "config_joint = config.copy(deep=True)\n",
    "config_joint.datasets.stack = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent unnecessary cluttering, we write it in a separate folder.\n",
    "path = Path(\"analysis_3d\")\n",
    "path.mkdir(exist_ok=True)\n",
    "config_joint.write(path=path / \"config_joint.yaml\", overwrite=True)\n",
    "config_stack.write(path=path / \"config_stack.yaml\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction and fitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked analysis\n",
    "\n",
    "We first show the steps for the stacked analysis and then repeat the same for the joint analysis later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading yaml file:\n",
    "config_stacked = AnalysisConfig.read(path=path / \"config_stack.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_stacked = Analysis(config_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select observations:\n",
    "analysis_stacked.get_observations()\n",
    "\n",
    "# run data reduction\n",
    "analysis_stacked.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one final dataset, which you can print and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_stacked.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_stacked.datasets[\"stacked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot a smooth counts map\n",
    "analysis_stacked.datasets[\"stacked\"].counts.smooth(\n",
    "    0.02 * u.deg\n",
    ").plot_interactive(add_cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the background map\n",
    "analysis_stacked.datasets[\"stacked\"].background_model.map.smooth(\n",
    "    0.02 * u.deg\n",
    ").plot_interactive(add_cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get an excess image with a few lines of code:\n",
    "counts = analysis_stacked.datasets[\"stacked\"].counts.sum_over_axes()\n",
    "background = analysis_stacked.datasets[\n",
    "    \"stacked\"\n",
    "].background_model.map.sum_over_axes()\n",
    "excess = counts - background\n",
    "excess.smooth(5).plot(stretch=\"sqrt\", add_cbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and fitting\n",
    "\n",
    "Now comes the interesting part of the analysis - choosing appropriate models for our source and fitting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit mask\n",
    "\n",
    "To select a certain energy range for the fit we can create a fit mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = analysis_stacked.datasets[\"stacked\"].counts.geom.get_coord()\n",
    "mask_energy = coords[\"energy\"] > 0.3 * u.TeV\n",
    "analysis_stacked.datasets[\"stacked\"].mask_safe.data &= mask_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit\n",
    "\n",
    "No we are ready for the actual likelihood fit. We first define the model as a combination of a point source with a powerlaw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=\"0.01 deg\", lat_0=\"0.01 deg\", frame=\"galactic\"\n",
    ")\n",
    "spectral_model = ExpCutoffPowerLawSpectralModel(\n",
    "    index=2,\n",
    "    amplitude=3e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n",
    "    reference=1.0 * u.TeV,\n",
    "    lambda_=0.1 / u.TeV,\n",
    ")\n",
    "\n",
    "sky_model = SkyModel(\n",
    "    spatial_model=spatial_model,\n",
    "    spectral_model=spectral_model,\n",
    "    name=\"gc-source\",\n",
    ")\n",
    "model = sky_model.copy()\n",
    "analysis_stacked.datasets[\"stacked\"].models = model\n",
    "\n",
    "analysis_stacked.datasets[\"stacked\"].background_model.norm.value = 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit = Fit(analysis_stacked.datasets)\n",
    "result = fit.run(optimize_opts={\"print_level\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model fit\n",
    "\n",
    "We check the model fit by computing and plotting a residual image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_stacked.datasets[\"stacked\"].plot_residuals(\n",
    "    method=\"diff/sqrt(model)\", vmin=-1, vmax=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the best fit spectrum. For that need to extract the covariance of the spectral parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model.spectral_model\n",
    "\n",
    "# set covariance on the spectral model\n",
    "covar = result.parameters.get_subcovariance(spec.parameters)\n",
    "spec.parameters.covariance = covar\n",
    "\n",
    "energy_range = [0.3, 10] * u.TeV\n",
    "spec.plot(energy_range=energy_range, energy_power=2)\n",
    "spec.plot_error(energy_range=energy_range, energy_power=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high value of the background normalisation `norm = 1.24` suggests that our model should be improved by adding a component for diffuse Galactic emission and at least one second point source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Galactic diffuse emission to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both models at the same time, our diffuse model (from the Fermi diffuse model) and our model for the central source. This time, in order to make it more realistic, we will consider an exponential cut off power law spectral model for the source. We will fit again the normalization and tilt of the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_model = SkyDiffuseCube.read(\n",
    "    \"$GAMMAPY_DATA/fermi-3fhl-gc/gll_iem_v06_gc.fits.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacked = analysis_stacked.datasets[\"stacked\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacked.models = SkyModels([model, diffuse_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see we have now two components in our model, and we can access them separately.\n",
    "print(dataset_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_combined = Fit([dataset_stacked])\n",
    "result_combined = fit_combined.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_combined.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the normalization of the background has vastly improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a comparison, we can the previous residual map (top) and the new one (bottom) with the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_stacked.datasets[\"stacked\"].plot_residuals(vmin=-1, vmax=1)\n",
    "dataset_stacked.plot_residuals(vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Flux Points\n",
    "\n",
    "Finally we compute flux points for the galactic center source. For this we first define an energy binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_edges = [0.3, 1, 3, 10] * u.TeV\n",
    "fpe = FluxPointsEstimator(\n",
    "    datasets=[dataset_stacked], e_edges=e_edges, source=\"gc-source-copy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "flux_points = fpe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the best fit model and flux points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points.table[\"is_ul\"] = flux_points.table[\"ts\"] < 4\n",
    "ax = flux_points.plot(energy_power=2)\n",
    "model.spectral_model.plot(ax=ax, energy_range=energy_range, energy_power=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint analysis\n",
    "\n",
    "In this section, we perform a joint analysis of the same data. Of course, joint fitting is considerably heavier than stacked one, and should always be handled with care. For bervity, we only show the analysis for a point source fitting without re-adding a diffuse component again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reduction:- joint fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read the yaml file from disc\n",
    "config_joint = AnalysisConfig.read(path=path / \"config_joint.yaml\")\n",
    "analysis_joint = Analysis(config_joint)\n",
    "\n",
    "# select observations:\n",
    "analysis_joint.get_observations()\n",
    "\n",
    "# run data reduction\n",
    "analysis_joint.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see there are 3 datasets now\n",
    "print(analysis_joint.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access each one by its name, eg:\n",
    "print(analysis_joint.datasets[\"obs_110380\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the model on each of the datasets\n",
    "model = sky_model.copy()\n",
    "for dataset in analysis_joint.datasets:\n",
    "    dataset.models = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now fit:\n",
    "fit_joint = Fit(analysis_joint.datasets)\n",
    "result_joint = fit_joint.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_joint.datasets.parameters.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information which parameter belongs to which dataset is not listed explicitly in the table (yet), but the order of parameters is conserved. You can always access the underlying object tree as well to get specific parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in analysis_joint.datasets:\n",
    "    print(dataset.background_model.norm.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting residuals\n",
    "Since we have multiple datasets, we can either look at a stacked residual map, or the residuals for each dataset. Each `gammapy.cube.MapDataset` object is equipped with a method called `gammapy.cube.MapDataset.plot_residuals()`, which displays the spatial and spectral residuals (computed as *counts-model*) for the dataset. Optionally, these can be normalized as *(counts-model)/model* or *(counts-model)/sqrt(model)*, by passing the parameter `norm='model` or `norm=sqrt_model`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the spectral residuals, we have to define a region for the spectral extraction\n",
    "region = CircleSkyRegion(spatial_model.position, radius=0.15 * u.deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To plot residuals for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in analysis_joint.datasets:\n",
    "    ax_image, ax_spec = dataset.plot_residuals(\n",
    "        region=region, vmin=-0.5, vmax=0.5, method=\"diff\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To plot a stacked residual map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.maps import Map\n",
    "\n",
    "# We need to stack on the full geometry, so we use to geom from the stacked counts map.\n",
    "residuals_stacked = Map.from_geom(analysis_stacked.datasets[0].counts.geom)\n",
    "\n",
    "for dataset in analysis_joint.datasets:\n",
    "    residuals = dataset.residuals()\n",
    "    residuals_stacked.stack(residuals)\n",
    "\n",
    "    residuals_stacked.sum_over_axes().smooth(\"0.08 deg\").plot(\n",
    "        vmin=-1, vmax=1, cmap=\"coolwarm\", add_cbar=True, stretch=\"linear\"\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Note that this notebook aims to show you the procedure of a 3D analysis using just a few observations and a cutted Fermi model. Results get much better for a more complete analysis considering the GPS dataset from the CTA First Data Challenge (DC-1) and also the CTA model for the Galactic diffuse emission, as shown in the next image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/DC1_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete tutorial notebook of this analysis is available to be downloaded in [GAMMAPY-EXTRA](https://github.com/gammapy/gammapy-extra) repository at https://github.com/gammapy/gammapy-extra/blob/master/analyses/cta_1dc_gc_3d.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Analyse the second source in the field of view: G0.9+0.1 and add it to the combined model.\n",
    "* Perform joint fit in more details - Add diffuse component, get flux points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
