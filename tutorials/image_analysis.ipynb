{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting 2D images with Gammapy\n",
    "\n",
    "Gammapy does not have any special handling for 2D images, but treats them as a subset of maps. Thus, classical 2D image analysis can be done in 2 independent ways: \n",
    "\n",
    "1. Using the sherpa package\n",
    "\n",
    "2. Within gammapy, by assuming 2D analysis to be a sub-set of the generalised `maps`. Thus, analysis should proceed exactly as demonstrated in [analysis_3d.ipynb](analysis_3d.ipynb), taking care of a few things that we mention in this tutorial\n",
    "\n",
    "We consider 2D `images` to be a special case of 3D `maps`, ie, maps with only one energy bin. This is a major difference while analysing in `sherpa`, where the `maps` must not contain any energy axis. In this tutorial, we do a classical image analysis using three example observations of the Galactic center region with CTA - i.e., study the source flux and morphology.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion\n",
    "\n",
    "from gammapy.detect import compute_lima_on_off_image\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import make_mean_psf\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.cube import (\n",
    "    SafeMaskMaker,\n",
    "    PSFKernel,\n",
    "    MapDataset,\n",
    "    MapDatasetMaker,\n",
    "    MapDatasetOnOff,\n",
    "    RingBackgroundMaker,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    ")\n",
    "from gammapy.modeling import Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare modeling input data\n",
    "\n",
    "### The counts, exposure and the background maps\n",
    "This is the same drill - use `DataStore` object to access the CTA observations and retrieve a list of observations by passing the observations IDs to the `.get_observations()` method, then use `MapMaker` to make the maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which data to use and print some information\n",
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/cta-1dc/index/gps/\")\n",
    "data_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store.obs_table[\"ONTIME\"].quantity.sum().to(\"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some observations from these dataset by hand\n",
    "obs_ids = [110380, 111140, 111159]\n",
    "observations = data_store.get_observations(obs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.1, 10] * u.TeV\n",
    "energy_axis = MapAxis.from_bounds(\n",
    "    emin.value, emax.value, 10, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even when doing a 2D analysis, it is better to use fine energy bins in the beginning and then sum them over. This is to ensure that the background shape can be approximated by a power law function in each energy bin. The `run_images()` can be used to compute maps in fine bins and then squash them to have one bin. This can be done by specifying `keep_dims = True`. This will compute a summed counts and background maps, and a spectral weighted exposure map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = MapDataset.create(geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "maker = MapDatasetMaker(selection=[\"counts\", \"exposure\", \"background\"])\n",
    "maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\"], offset_max=4.0 * u.deg)\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for obs in observations:\n",
    "    cutout = stacked.cutout(obs.pointing_radec, width=\"8 deg\")\n",
    "    dataset = maker.run(cutout, obs)\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "    datasets.append(dataset)\n",
    "    stacked.stack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = PowerLaw2SpectralModel(index=2)\n",
    "dataset_2d = stacked.to_image(spectrum=spectrum)\n",
    "\n",
    "maps2D = {\n",
    "    \"counts\": dataset_2d.counts,\n",
    "    \"exposure\": dataset_2d.exposure,\n",
    "    \"background\": dataset_2d.background_model.map,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a typical 2D analysis, using an energy dispersion usually does not make sense. A PSF map can be made as in the regular 3D case, taking care to weight it properly with the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean PSF\n",
    "geom2d = maps2D[\"exposure\"].geom\n",
    "src_pos = SkyCoord(0, 0, unit=\"deg\", frame=\"galactic\")\n",
    "table_psf = make_mean_psf(observations, src_pos)\n",
    "\n",
    "table_psf_2d = table_psf.table_psf_in_energy_band(\n",
    "    (emin, emax), spectrum=spectrum\n",
    ")\n",
    "\n",
    "# PSF kernel used for the model convolution\n",
    "psf_kernel = PSFKernel.from_table_psf(\n",
    "    table_psf_2d, geom2d, max_radius=\"0.3 deg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the analysis proceeds as usual. Just take care to use the proper geometry in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = CircleSkyRegion(center=src_pos, radius=0.6 * u.deg)\n",
    "mask = Map.from_geom(geom2d, data=geom2d.region_mask([region]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the source\n",
    "\n",
    "This is the important thing to note in this analysis. Since modeling and fitting in `~gammapy.maps` needs to have a combination of spectral models, we have to use a dummy Powerlaw as for the spectral model and fix its index to 2. Since we are interested only in the integral flux, we will use the `PowerLaw2SpectralModel` model which directly fits an integral flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=\"0.01 deg\", lat_0=\"0.01 deg\", frame=\"galactic\"\n",
    ")\n",
    "spectral_model = PowerLaw2SpectralModel(\n",
    "    emin=emin, emax=emax, index=2.0, amplitude=\"3e-12 cm-2 s-1\"\n",
    ")\n",
    "model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)\n",
    "model.parameters[\"index\"].frozen = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the background\n",
    "\n",
    "Gammapy fitting framework assumes the background to be an integrated model.\n",
    "Thus, we will define the background as a model, and freeze its parameters for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = BackgroundModel(maps2D[\"background\"])\n",
    "background_model.parameters[\"norm\"].frozen = True\n",
    "background_model.parameters[\"tilt\"].frozen = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MapDataset(\n",
    "    models=model,\n",
    "    counts=maps2D[\"counts\"],\n",
    "    exposure=maps2D[\"exposure\"],\n",
    "    background_model=background_model,\n",
    "    mask_fit=mask,\n",
    "    psf=psf_kernel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit = Fit([dataset])\n",
    "result = fit.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual best-fit parameters, do a print on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.parameters.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.parameters.correlation[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Ring Background Analysis\n",
    "\n",
    "No we repeat the same analysis but using a classical ring background estimation. We define an exclusion mask and then use the `~gammapy.cube.RingBackgroundMaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_image = geom.to_image().to_cube([energy_axis.squash()])\n",
    "\n",
    "regions = CircleSkyRegion(center=spatial_model.position, radius=0.3 * u.deg)\n",
    "\n",
    "exclusion_mask = Map.from_geom(geom_image)\n",
    "exclusion_mask.data = geom_image.region_mask([regions], inside=False)\n",
    "exclusion_mask.sum_over_axes().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ring_maker = RingBackgroundMaker(\n",
    "    r_in=\"0.3 deg\", width=\"0.3 deg\", exclusion_mask=exclusion_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacked_on_off = MapDatasetOnOff.create(geom=geom_image)\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_image = dataset.to_image()\n",
    "    dataset_on_off = ring_maker.run(dataset_image)\n",
    "    stacked_on_off.stack(dataset_on_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the estimate of the ring background we compute a Li&Ma significance image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = geom.pixel_scales[0].to(\"deg\")\n",
    "# Using a convolution radius of 0.05 degrees\n",
    "theta = 0.15 * u.deg / scale\n",
    "tophat = Tophat2DKernel(theta)\n",
    "tophat.normalize(\"peak\")\n",
    "\n",
    "lima_maps = compute_lima_on_off_image(\n",
    "    stacked_on_off.counts,\n",
    "    stacked_on_off.counts_off,\n",
    "    stacked_on_off.acceptance,\n",
    "    stacked_on_off.acceptance_off,\n",
    "    tophat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_map = lima_maps[\"significance\"]\n",
    "excess_map = lima_maps[\"excess\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is what the excess and significance look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot(221, projection=significance_map.geom.wcs)\n",
    "ax2 = plt.subplot(222, projection=excess_map.geom.wcs)\n",
    "\n",
    "ax1.set_title(\"Significance map\")\n",
    "significance_map.get_image_by_idx((0,)).plot(ax=ax1, add_cbar=True)\n",
    "\n",
    "ax2.set_title(\"Excess map\")\n",
    "excess_map.get_image_by_idx((0,)).plot(ax=ax2, add_cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we take a look at the signficance distribution outside the exclusion region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2D mask for the images\n",
    "significance_map_off = significance_map * exclusion_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_all = significance_map.data[np.isfinite(significance_map.data)]\n",
    "significance_off = significance_map_off.data[\n",
    "    np.isfinite(significance_map_off.data)\n",
    "]\n",
    "\n",
    "plt.hist(\n",
    "    significance_all,\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"red\",\n",
    "    label=\"all bins\",\n",
    "    bins=21,\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    significance_off,\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    label=\"off bins\",\n",
    "    bins=21,\n",
    ")\n",
    "\n",
    "# Now, fit the off distribution with a Gaussian\n",
    "mu, std = norm.fit(significance_off)\n",
    "x = np.linspace(-8, 8, 50)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, lw=2, color=\"black\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Significance\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-5, 1)\n",
    "xmin, xmax = np.min(significance_all), np.max(significance_all)\n",
    "plt.xlim(xmin, xmax)\n",
    "\n",
    "print(f\"Fit results: mu = {mu:.2f}, std = {std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Update the exclusion mask in the ring background example by thresholding the significance map and re-run the background estimator \n",
    "1. Plot residual maps as done in the [analysis_3d](analysis_3d.ipynb) notebook\n",
    "1. Iteratively add and fit sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
