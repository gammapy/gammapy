{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light curve estimation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Knowledge of the high level interface to perform data reduction, see [first gammapy analysis with the high level interface tutorial](analysis_1.ipynb)\n",
    "\n",
    "## Context\n",
    "\n",
    "This tutorial presents how light curve extraction is performed in gammapy, i.e. how to measure the flux of a source\n",
    "in different time bins.\n",
    "\n",
    "Cherenkov telescopes usually work with observing runs and distribute data according to this basic time interval. A typical use case is to look for variability of a source on various time binnings: observation run-wise binning, nightly, weekly etc.\n",
    "\n",
    "**Objective: The Crab nebula is not known to be variable at TeV energies, so we expect constant brightness within statistical and systematic errors. Compute per-observation and nightly fluxes of the four Crab nebula observations from the [H.E.S.S. first public test data release](https://www.mpi-hd.mpg.de/hfm/HESS/pages/dl3-dr1/) to check it.**\n",
    "\n",
    "## Proposed approach\n",
    "\n",
    "We will demonstrate how to compute a `~gammapy.time.LightCurve` from 3D reduced datasets (`~gammapy.cube.MapDataset`) as well as 1D ON-OFF spectral datasets (`~gammapy.spectrum.SpectrumDatasetOnOff`). \n",
    "\n",
    "The data reduction will be performed with the high level interface for the data reduction. Then we will use the `~gammapy.time.LightCurveEstimator` class, which  is able to extract a light curve independently of the dataset type. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As usual, we'll start with some general imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import logging\n",
    "\n",
    "from astropy.time import Time\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import gammapy specific classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.modeling.models import PowerLawSpectralModel\n",
    "from gammapy.modeling.models import PointSpatialModel\n",
    "from gammapy.modeling.models import SkyModel, Models\n",
    "from gammapy.time import LightCurveEstimator\n",
    "from gammapy.analysis import Analysis, AnalysisConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis configuration \n",
    "For the 1D and 3D extraction, we will use the same CrabNebula configuration than in the notebook analysis_1.ipynb using the high level interface of Gammapy.\n",
    "\n",
    "From the high level interface, the data reduction for those observations is performed as followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the 3D analysis configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_3d = AnalysisConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the data selection\n",
    "\n",
    "Here we use the Crab runs from the HESS DL3 data release 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_3d.observations.obs_ids = [23523, 23526, 23559, 23592]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the dataset geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a 3D analysis\n",
    "conf_3d.datasets.type = \"3d\"\n",
    "\n",
    "# We want to extract the data by observation and therefore to not stack them\n",
    "conf_3d.datasets.stack = False\n",
    "\n",
    "# Here is the WCS geometry of the Maps\n",
    "conf_3d.datasets.geom.wcs.skydir = dict(\n",
    "    frame=\"icrs\", lon=83.63308 * u.deg, lat=22.01450 * u.deg\n",
    ")\n",
    "conf_3d.datasets.geom.wcs.binsize = 0.02 * u.deg\n",
    "conf_3d.datasets.geom.wcs.fov = dict(width=1 * u.deg, height=1 * u.deg)\n",
    "\n",
    "# We define a value for the IRF Maps binsize\n",
    "conf_3d.datasets.geom.wcs.binsize_irf = 0.2 * u.deg\n",
    "\n",
    "# Define energy binning for the Maps\n",
    "conf_3d.datasets.geom.axes.energy = dict(\n",
    "    min=0.7 * u.TeV, max=10 * u.TeV, nbins=5\n",
    ")\n",
    "conf_3d.datasets.geom.axes.energy_true = dict(\n",
    "    min=0.3 * u.TeV, max=20 * u.TeV, nbins=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the 3D data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_3d = Analysis(conf_3d)\n",
    "analysis_3d.get_observations()\n",
    "analysis_3d.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model to be used\n",
    "\n",
    "Here we don't try to fit the model parameters to the whole dataset, but we use predefined values instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_position = SkyCoord(ra=83.63308, dec=22.01450, unit=\"deg\")\n",
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=target_position.ra, lat_0=target_position.dec, frame=\"icrs\"\n",
    ")\n",
    "\n",
    "spectral_model = PowerLawSpectralModel(\n",
    "    index=2.702,\n",
    "    amplitude=4.712e-11 * u.Unit(\"1 / (cm2 s TeV)\"),\n",
    "    reference=1 * u.TeV,\n",
    ")\n",
    "\n",
    "sky_model = SkyModel(\n",
    "    spatial_model=spatial_model, spectral_model=spectral_model, name=\"crab\"\n",
    ")\n",
    "# Now we freeze these parameters that we don't want the light curve estimator to change\n",
    "sky_model.parameters[\"index\"].frozen = True\n",
    "sky_model.parameters[\"lon_0\"].frozen = True\n",
    "sky_model.parameters[\"lat_0\"].frozen = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign them the model to be fitted to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models([sky_model])\n",
    "analysis_3d.set_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Curve estimation: by observation\n",
    "\n",
    "We can now create the light curve estimator.\n",
    "\n",
    "We pass it the list of datasets and the name of the model component for which we want to build the light curve. \n",
    "We can optionally ask for parameters reoptimization during fit, that is most of the time to fit background normalization in each time bin. \n",
    "\n",
    "If we don't set any time interval, the `~gammapy.time.LightCurveEstimator` is determines the flux of each dataset and places it at the corresponding time in the light curve. \n",
    "Here one dataset equals to one observing run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_maker_3d = LightCurveEstimator(\n",
    "    analysis_3d.datasets, source=\"crab\", reoptimize=False\n",
    ")\n",
    "lc_3d = lc_maker_3d.run(e_ref=1 * u.TeV, e_min=1.0 * u.TeV, e_max=10.0 * u.TeV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LightCurve object contains a table which we can explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_3d.table[\"time_min\", \"time_max\", \"e_min\", \"e_max\", \"flux\", \"flux_err\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the light curve extraction in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the 1D analysis configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_1d = AnalysisConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the data selection\n",
    "\n",
    "Here we use the Crab runs from the HESS DL3 data release 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_1d.observations.obs_ids = [23523, 23526, 23559, 23592]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the dataset geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a 1D analysis\n",
    "conf_1d.datasets.type = \"1d\"\n",
    "\n",
    "# We want to extract the data by observation and therefore to not stack them\n",
    "conf_1d.datasets.stack = False\n",
    "\n",
    "# Here we define the ON region and make sure that PSF leakage is corrected\n",
    "conf_1d.datasets.on_region = dict(\n",
    "    frame=\"icrs\",\n",
    "    lon=83.63308 * u.deg,\n",
    "    lat=22.01450 * u.deg,\n",
    "    radius=0.1 * u.deg,\n",
    ")\n",
    "conf_1d.datasets.containment_correction = True\n",
    "\n",
    "# Finally we define the energy binning for the spectra\n",
    "conf_1d.datasets.geom.axes.energy = dict(\n",
    "    min=0.7 * u.TeV, max=10 * u.TeV, nbins=20\n",
    ")\n",
    "conf_1d.datasets.geom.axes.energy_true = dict(\n",
    "    min=0.3 * u.TeV, max=20 * u.TeV, nbins=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the 1D data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_1d = Analysis(conf_1d)\n",
    "analysis_1d.get_observations()\n",
    "analysis_1d.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model to be used\n",
    "\n",
    "Here we don't try to fit the model parameters to the whole dataset, but we use predefined values instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_position = SkyCoord(ra=83.63308, dec=22.01450, unit=\"deg\")\n",
    "spatial_model = PointSpatialModel(\n",
    "    lon_0=target_position.ra, lat_0=target_position.dec, frame=\"icrs\"\n",
    ")\n",
    "\n",
    "spectral_model = PowerLawSpectralModel(\n",
    "    index=2.702,\n",
    "    amplitude=4.712e-11 * u.Unit(\"1 / (cm2 s TeV)\"),\n",
    "    reference=1 * u.TeV,\n",
    ")\n",
    "\n",
    "sky_model = SkyModel(\n",
    "    spatial_model=spatial_model, spectral_model=spectral_model, name=\"crab\"\n",
    ")\n",
    "# Now we freeze these parameters that we don't want the light curve estimator to change\n",
    "sky_model.parameters[\"index\"].frozen = True\n",
    "sky_model.parameters[\"lon_0\"].frozen = True\n",
    "sky_model.parameters[\"lat_0\"].frozen = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the model to be fitted to each dataset. We can use the same `~gammapy.modeling.models.SkyModel` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models([sky_model])\n",
    "analysis_1d.set_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_maker_1d = LightCurveEstimator(\n",
    "    analysis_1d.datasets, source=\"crab\", reoptimize=False\n",
    ")\n",
    "lc_1d = lc_maker_1d.run(e_ref=1 * u.TeV, e_min=1.0 * u.TeV, e_max=10.0 * u.TeV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results\n",
    "\n",
    "Finally we compare the result for the 1D and 3D lightcurve in a single figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lc_1d.plot(marker=\"o\", label=\"1D\")\n",
    "lc_3d.plot(ax=ax, marker=\"o\", label=\"3D\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Night-wise LC estimation\n",
    "\n",
    "Here we want to extract a night curve per night. We define the time intervals that cover the three nights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals = [\n",
    "    Time([53343.5, 53344.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([53345.5, 53346.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([53347.5, 53348.5], format=\"mjd\", scale=\"utc\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the LC on the time intervals defined above, we pass the `LightCurveEstimator` the list of time intervals. \n",
    "\n",
    "Internally, datasets are grouped per time interval and a flux extraction is performed for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_maker_1d = LightCurveEstimator(\n",
    "    analysis_1d.datasets,\n",
    "    time_intervals=time_intervals,\n",
    "    source=\"crab\",\n",
    "    reoptimize=False,\n",
    ")\n",
    "\n",
    "nightwise_lc = lc_maker_1d.run(\n",
    "    e_ref=1 * u.TeV, e_min=1.0 * u.TeV, e_max=10.0 * u.TeV\n",
    ")\n",
    "\n",
    "nightwise_lc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "When sources are bight enough to look for variability at small time scales, the per-observation time binning is no longer relevant. One can easily extend the light curve estimation approach presented above to any time binning. This is demonstrated in the [following tutorial](light_curve_flare.ipynb) which shows the extraction of the lightcurve of an AGN flare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
