{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make template background model\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this tutorial, we will create a template background model from scratch. Often, background models are pre-computed and provided for analysis, but it's educational to see how the sausage is made.\n",
    "\n",
    "We will use the \"off observations\", i.e. those without significant gamma-ray emission sources in the field of view from the [H.E.S.S. first public test data release](https://www.mpi-hd.mpg.de/hfm/HESS/pages/dl3-dr1/). This model could then be used in the analysis of sources from that dataset (not done here).\n",
    "\n",
    "We will make a background model that is radially symmetric in the field of view, i.e. only depends on field of view offset angle and energy. At the end, we will save the model in the `BKG_2D` as defined in the [spec](https://gamma-astro-data-formats.readthedocs.io/en/latest/irfs/full_enclosure/bkg/index.html).\n",
    "\n",
    "Note that this is just a quick and dirty example. Actual background model production is done with more sophistication usually using 100s or 1000s of off runs, e.g. concerning non-radial symmetries, binning and smoothing of the distributions, and treating other dependencies such as zenith angle, telescope configuration or optical efficiency. Another aspect not shown here is how to use AGN observations to make background models, by cutting out the part of the field of view that contains gamma-rays from the AGN.\n",
    "\n",
    "We will mainly be using the following classes:\n",
    "        \n",
    "* [gammapy.data.DataStore](http://docs.gammapy.org/dev/api/gammapy.data.DataStore.html) to load the runs to use to build the bkg model.\n",
    "* [gammapy.irf.Background2D](http://docs.gammapy.org/dev/api/gammapy.irf.Background2D.html) to represent and write the background model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, we start the notebook with some setup and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammapy.extern.pathlib import Path\n",
    "from gammapy.utils.nddata import sqrt_space\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import Background2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select off data\n",
    "\n",
    "We start by selecting the observations used to estimate the background model.\n",
    "\n",
    "In this case, we just take all \"off runs\" as defined in the observation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = DataStore.from_dir(\"../datasets/hess-dl3-dr1\")\n",
    "mask = data_store.obs_table[\"OBS_SUBSET_TAG\"] == \"offdata\"\n",
    "obs_ids = data_store.obs_table[\"OBS_ID\"][mask].data\n",
    "observations = data_store.obs_list(obs_ids)\n",
    "print(\"Number of observations:\", len(observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate background model\n",
    "\n",
    "The background model we will estimate is a differential background rate model in unit `s-1 MeV-1 sr-1` as a function of reconstructed energy and field of fiew offset.\n",
    "\n",
    "We estimate it by histogramming off data events and then smoothing a bit (not using a good method) to get a less noisy estimate. To get the differential rate, we divide by observation time and also take bin sizes into account to get the rate per energy and solid angle. So overall we fill two arrays called `counts` and `exposure` with `exposure` filled so that `background_rate = counts / exposure` will give the final background rate we're interested in.\n",
    "\n",
    "The processing can be done either one observation at a time, or first for counts and then for exposure. Either way is fine. Here we do one observation at a time, starting with empty histograms and then accumulating counts and exposure. Since this is a multi-step algorithm, we put the code to do this computation in a `BackgroundModelEstimator` class.\n",
    "\n",
    "This functionality was already in Gammapy previously, and will be added back again soon, after `gammapy.irf` has been restructured and improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroundModelEstimator(object):\n",
    "    def __init__(self, ebounds, offset):\n",
    "        self.counts = self._make_bkg2d(ebounds, offset, unit=\"\")\n",
    "        self.exposure = self._make_bkg2d(ebounds, offset, unit=\"s MeV sr\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_bkg2d(ebounds, offset, unit):\n",
    "        ebounds = ebounds.to(\"MeV\")\n",
    "        offset = offset.to(\"deg\")\n",
    "        shape = len(ebounds) - 1, len(offset) - 1\n",
    "        return Background2D(\n",
    "            energy_lo=ebounds[:-1],\n",
    "            energy_hi=ebounds[1:],\n",
    "            offset_lo=offset[:-1],\n",
    "            offset_hi=offset[1:],\n",
    "            data=np.zeros(shape) * u.Unit(unit),\n",
    "        )\n",
    "\n",
    "    def run(self, observations):\n",
    "        for obs in observations:\n",
    "            self.fill_counts(obs)\n",
    "            self.fill_exposure(obs)\n",
    "\n",
    "    def fill_counts(self, obs):\n",
    "        events = obs.events\n",
    "        data = self.counts.data\n",
    "        counts = np.histogram2d(\n",
    "            x=events.energy.to(\"MeV\"),\n",
    "            y=events.offset.to(\"deg\"),\n",
    "            bins=(data.axes[0].bins, data.axes[1].bins),\n",
    "        )[0]\n",
    "        data.data += counts\n",
    "\n",
    "    def fill_exposure(self, obs):\n",
    "        data = self.exposure.data\n",
    "        energy_width = data.axes[0].bin_width\n",
    "        offset = data.axes[1].nodes\n",
    "        offset_width = data.axes[1].bin_width\n",
    "        solid_angle = 2 * np.pi * offset * offset_width\n",
    "        time = obs.observation_time_duration\n",
    "        exposure = time * energy_width[:, None] * solid_angle[None, :]\n",
    "        data.data += exposure\n",
    "\n",
    "    @property\n",
    "    def background_rate(self):\n",
    "        rate = deepcopy(self.counts)\n",
    "        rate.data.data /= self.exposure.data.data\n",
    "        return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ebounds = np.logspace(-1, 2, 20) * u.TeV\n",
    "offset = sqrt_space(start=0, stop=2.5, num=10) * u.deg\n",
    "estimator = BackgroundModelEstimator(ebounds, offset)\n",
    "# observations = observations[:3]  # To run more quickly for debugging\n",
    "estimator.run(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check background model\n",
    "\n",
    "Let's make a few plots to see what our background model looks like:\n",
    "\n",
    "1. Acceptance curve (background rate as a function of field of view offset for a given energy)\n",
    "1. Rate spectrum (background rate as a function of energy for a given offset)\n",
    "1. Rate image (background rate as a function of energy and offset)\n",
    "\n",
    "### Acceptance curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "hist = estimator.background_rate.data.data\n",
    "print(hist.shape)\n",
    "print(ebounds.shape)\n",
    "print(offset.shape)\n",
    "plt.pcolormesh(ebounds, offset, hist.T.value, norm=LogNorm())\n",
    "plt.semilogx()\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Energy (TeV)\")\n",
    "plt.ylabel(\"Offset (deg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could save the background model to a file like this\n",
    "# estimator.background_rate.to_fits().writeto('background_model.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "**The rest of the notebook is still the old version.\n",
    "Needs to be removed or rewritten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one of the background models from file\n",
    "filename = scratch_dir / \"smooth_background_2D_group_000_table.fits.gz\"\n",
    "model = EnergyOffsetBackgroundModel.read(str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = model.bg_rate.offset_bin_center\n",
    "energies = model.bg_rate.energy\n",
    "iE = 6\n",
    "\n",
    "x = offset\n",
    "y = model.bg_rate.data[iE, :]\n",
    "plt.plot(x, y, label=\"bkg model smooth\")\n",
    "title = (\n",
    "    \"energy band: \"\n",
    "    + str(\"%.2f\" % energies[iE].value)\n",
    "    + \"-\"\n",
    "    + str(\"%.2f\" % energies[iE + 1].value)\n",
    "    + \" TeV\"\n",
    ")\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Offset (degree)\")\n",
    "plt.ylabel(\"Bkg rate (MeV-1 s-1 sr-1)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background rate spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = energies.log_centers\n",
    "y = model.bg_rate.data[:, 10]\n",
    "plt.loglog(x, y, label=\"bkg model smooth\")\n",
    "plt.title(\"offset: \" + str(\"%.2f\" % offset[10].value) + \" deg\")\n",
    "plt.xlabel(\"Energy (TeV)\")\n",
    "plt.ylabel(\"Bkg rate (MeV-1 s-1 sr-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background rate image with energy and offset axes\n",
    "\n",
    "It doesn't look good in this case.\n",
    "To do this well, you need to use more off or AGN runs to build the background model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bg_rate.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make new index table\n",
    "\n",
    "Here we first copy the dataset of the 4 crab runs from gammapy-extra in a new directory containing the data you will use for the analysis. \n",
    "\n",
    "We use the same dataset to produce the bkg or for the analysis. Of course normally you produce the bkg model using thousands of AGN runs not the 4 Crab test runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new hdu table in your dataset directory that contains the link to the acceptance curve to use to build the bkg model in your cube analysis\n",
    "data_dir = make_fresh_dir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataStore.from_dir(\"$GAMMAPY_EXTRA/datasets/hess-crab4-hd-hap-prod2\")\n",
    "ds.copy_obs(ds.obs_table, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hdu_table in this directory contains no link to a bkg model for each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = DataStore.from_dir(data_dir)\n",
    "data_store.hdu_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to produce a background image or background cube we have to create a hdu table that contains for each observation a link to the bkg model to use depending of the observation conditions of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the background directory in the one where is located the hdu table, here data\n",
    "shutil.move(str(scratch_dir), str(data_dir))\n",
    "\n",
    "# Create the new hdu table with a link to the background model\n",
    "group_filename = data_dir / \"background/group-def.fits\"\n",
    "\n",
    "# relat_path= (scratch_dir.absolute()).relative_to(data_dir.absolute())\n",
    "hdu_index_table = bgmaker.make_total_index_table(\n",
    "    data_store=data_store,\n",
    "    modeltype=\"2D\",\n",
    "    out_dir_background_model=scratch_dir,\n",
    "    filename_obs_group_table=str(group_filename),\n",
    "    smooth=False,\n",
    ")\n",
    "\n",
    "# Write the new hdu table\n",
    "filename = data_dir / \"hdu-index.fits.gz\"\n",
    "hdu_index_table.write(str(filename), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_index_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdu_index_table[-4][\"FILE_DIR\"], \" \", hdu_index_table[-4][\"FILE_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdu_index_table[-1][\"FILE_DIR\"], \" \", hdu_index_table[-1][\"FILE_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Use real AGN run\n",
    "- Change the binning for the grouping: thinner zenithal bin, add efficiency binning ....\n",
    "- Change the energy binning (ebounds) and the offset (offset) used to compute the acceptance curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "In this tutorial we have created a template background model in the `bkg_2d` format, i.e. with offset and energy axes (see [spec](http://gamma-astro-data-formats.readthedocs.io/en/latest/irfs/background/index.html#bkg-2d-format)).\n",
    "\n",
    "In future tutorials, we will use this background model as one of the model components for source analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
