#! /usr/bin/python
import argparse
import os.path
import sys

import numpy as np
from scipy import signal
from scipy import interpolate,ndimage

import pyfits
import pywcs

def gauss_kern(radius):
    """ Returns a normalized 2D gauss kernel array for convolutions """
    sizex = int(8*radius)
    sizey = int(8*radius)
    radius = float(radius)
    xc=0.5*sizex
    yc=0.5*sizey
    x, y = np.mgrid[0:sizex-1, 0:sizey-1]
    x-=xc
    y-=yc
    x= x/radius
    y= y/radius
    g = np.exp(-0.5*(x**2+y**2))
    return g / (2*np.pi*radius**2)   #g.sum()

def DOG_kern(radius,alpha):
    """ Returns a normalized 2D gauss kernel array for convolutions """
    sizex = int(8*alpha*radius)
    sizey = int(8*alpha*radius)
    radius = float(radius)
    xc=0.5*sizex
    yc=0.5*sizey
    x, y = np.mgrid[0:sizex-1, 0:sizey-1]
    x-=xc
    y-=yc
    x1= x/radius
    y1= y/radius
    g1 = np.exp(-0.5*(x1**2+y1**2))
    g1 = g1/(2*np.pi*radius**2)   #g1.sum()
    x1= x1/alpha
    y1= y1/alpha
    g2 = np.exp(-0.5*(x1**2+y1**2))
    g2 = g2/(2*np.pi*radius**2*alpha**2)   #g2.sum()
    return g1-g2

class CWT:
    """ Initialization of wavelet family
        nscales is the number of scales considered
        alpha is the base scaling factor
        min_scale is the first scale used"""
    def __init__(self, nscales, alpha, min_scale):
        self.kernbase=dict()
        self.scales = dict()
        self.nscale = nscales
        self.alpha=alpha
        for ns in np.arange(0,nscales):
            self.scales[ns] = (alpha**(ns))*min_scale
            self.kernbase[ns]=DOG_kern((alpha**(ns))*min_scale,alpha)

        self.scale_array = (alpha**(np.arange(0,nscales)))*min_scale
 
        self.kern_approx=gauss_kern((alpha**nscales)*min_scale)
 
#        self.transform = dict()
#        self.error = dict()
#        self.support = dict()
        
        self.header = None
        self.wcs = None

    # should check that image and bkg are consistent 
    def setdata(self,image,back):
        self.image = image -0.0         
        self.nx,self.ny = self.image.shape
        self.filter = np.zeros((self.nx,self.ny)) 
        self.bkg = back -0.0   # hack because of some bug with old version of fft in numpy
        self.model = np.zeros((self.nx,self.ny)) 
        self.approx =  np.zeros((self.nx,self.ny))
 
        self.transform = np.zeros((self.nscale,self.nx,self.ny))
        self.error = np.zeros((self.nscale,self.nx,self.ny)) 
        self.support = np.zeros((self.nscale,self.nx,self.ny))
       
    
    # should check the existence of extensions
    def setfile(self,filename):
        # Open fits files
        hdulist = pyfits.open(filename)
        self.setdata(hdulist[0].data,hdulist['NormOffMap'].data)
        self.header = hdulist[0].header
        self.wcs = pywcs.WCS(self.header)

    # the transform itself
    def doTransform(self):
        tot_bkg =  self.model + self.bkg + self.approx
        excess = self.image - tot_bkg
        for key,kern in self.kernbase.iteritems():   # works for python < 3
            self.transform[key] = signal.fftconvolve(excess, kern, mode='same')
            self.error[key] = np.sqrt(signal.fftconvolve(tot_bkg, kern**2, mode='same'))
            
        self.approx = signal.fftconvolve(self.image-self.model-self.bkg, self.kern_approx, mode='same')
        self.approx_bkg = signal.fftconvolve(self.bkg, self.kern_approx, mode='same')
                 
    # Compute the multiresolution support with hard sigma clipping 
    # imposing a minimum significance on a connex region of significant pixels (i.e. source detection) 
    def ComputeSupportPeak(self,nsigma = 2.0,nsigmap=4.0,remove_isolated=True):
        # should check that transform has been performed
        sig = self.transform / self.error
        for key in self.scales.keys():
            tmp = sig[key]>nsigma
            # produce a list of connex structures in the support
            l,n = ndimage.label(tmp)
            for id in range(1,n):
                index = np.where(l==id)
                if remove_isolated:                    
                    if index[0].size==1:
                        tmp[index] *= 0.0      # Remove isolated pixels from support
                signif = sig[key][index]
                if signif.max() < nsigmap:     # Remove significant pixels island from support 
                    tmp[index] *= 0.0          # if max does not reach maximal significance

            self.support[key] += tmp
            self.support[key] = self.support[key]>0.
            
    def InverseTransform(self):
        res = np.sum(self.support*self.transform,0)
        self.filter += res * (res>0)
        self.model = self.filter 
        return res

    def IterativeFilterPeak(self, nsigma=3.0, nsigmap=4.0,niter=2):
        var_ratio=0.0
        for iiter in range(niter):
            self.doTransform()
            self.ComputeSupportPeak(nsigma,nsigmap)
            res = self.InverseTransform()
            residual = self.image-(self.model+self.approx)
            tmp_var = residual.var()
            if iiter > 0:
                var_ratio = abs((self.residual_var -tmp_var)/self.residual_var)
                if var_ratio < results.convergence:
                    print "Convergence reached at iteration ",iiter+1
                    return res
            self.residual_var = tmp_var
        print "Convergence not formally reached at iteration ",iiter+1
        print "Final convergence parameter ",var_ratio, " objective was : ",results.convergence 
        return res

    def max_scale_image(self):
        maximum=np.argmax(self.transform,0)
        return  self.scale_array[maximum]*(self.support.sum(0)>0)

    def save_filter(self,filename="res.fits",clob=True):
        hdu=pyfits.PrimaryHDU(self.filter,self.header)
        hdu.writeto(filename,clobber=clob)
        pyfits.append(filename,self.approx,self.header)
        pyfits.append(filename,self.filter+self.approx,self.header)
        pyfits.append(filename,self.max_scale_image(),self.header)

if __name__=='__main__':
    parser = argparse.ArgumentParser(description='Compute filtered HESS image using Continuous Wavelet Transform (CWT)')

    parser.add_argument('infile', action="store", help='Input file name')
    parser.add_argument('outfile', action="store", help='Output file name')
    # Wavelet scales to be used
    parser.add_argument('--nscale', help='Number of scales to be used.', default=6,type=int)
    parser.add_argument('--minscale', help='Minimum scale to use.', default=6.0,type=float)
    parser.add_argument('--step', help='Geometric step between to successive Gaussian.', default=1.3,type=float)
    # Detection thresholds
    parser.add_argument('--thresh', help='Significance threshold for pixel detection.', default=3.0,type=float)
    parser.add_argument('--detect', help='Significance threshold for source detection.', default=5.0,type=float)
    parser.add_argument('--niter', help='Maximum number of iteration to be performed.', default =5,type=int)
    parser.add_argument('--convergence', help='Convergence parameter.', default =1e-5,type=float)

    parser.add_argument('--clobber', action='store_true', help='Clobber output file.', default=False)

    results=parser.parse_args()

    if  os.path.isfile(results.outfile) and not results.clobber:
        print "output file exists and clobber is False"
        sys.exit()

    mycwt = CWT(results.nscale,results.step,results.minscale)
    mycwt.setfile(results.infile)
    rec = mycwt.IterativeFilterPeak(results.thresh,results.detect,results.niter)
    mycwt.save_filter(results.outfile,results.clobber)

