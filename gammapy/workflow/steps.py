# Licensed under a 3-clause BSD style license - see LICENSE.rst
"""classes containing the workflow steps supported by the high level interface"""

import abc
import collections.abc
import logging
import os
import numpy as np
from astropy.coordinates import SkyCoord
from astropy.table import Table
from regions import CircleSkyRegion
from gammapy.data import DataStore
from gammapy.datasets import Datasets, FluxPointsDataset, MapDataset, SpectrumDataset
from gammapy.estimators import (
    ExcessMapEstimator,
    FluxPointsEstimator,
    LightCurveEstimator,
)
from gammapy.makers import (
    DatasetsMaker,
    FoVBackgroundMaker,
    MapDatasetMaker,
    ReflectedRegionsBackgroundMaker,
    RingBackgroundMaker,
    SafeMaskMaker,
    SpectrumDatasetMaker,
)
from gammapy.maps import Map, MapAxis, RegionGeom, WcsGeom
from gammapy.utils.pbar import progress_bar
from gammapy.utils.scripts import make_name, make_path, requires_module

try:
    import ray
except ImportError:
    ray = None  # Fallback if ray is not available


class Product:
    """Represents a product generated during a processing step in a workflow."""

    def __init__(self, name=None, step_name=None, data=None):
        """
        Initialize a Product instance.

        Parameters
        ----------
        name : str, optional
            A label or identifier for the product.
        step_name : str, optional
            The name of the processing step that produced this product.
        data : object, optional
            The data associated with the product. Can be any object, including a Ray ObjectRef.
        """

        self.step_name = step_name
        self.data = data
        if name is None:
            name = make_name()
        self.name = name
        self.pid = make_name()  # unique id generated for each product

    @property
    @requires_module("ray")
    def needs_update(self):
        """
        Check whether the product's data needs to be updated.
        Only available if `ray` is available.

        Returns
        -------
        bool
            True if `data` is a `ray.ObjectRef`, indicating that the data is a reference
            to a remote object and may need to be resolved or updated.
        """
        return isinstance(self.data, ray.ObjectRef)

    def copy(self):
        """
        Create a shallow copy of the product with a new unique identifier.

        Returns
        -------
        Product
            A new `Product` instance with the same `tag`, `step_name`, and `data`,
            but a newly generated `name`.
        """
        return self.__class__(name=self.name, step_name=self.step_name, data=self.data)


class Products(collections.abc.MutableSequence):
    """
    Container for managing a list of `Product` instances generated by the workflow.

    This class provides list-like behavior and additional utilities for
    selecting, copying, and updating products, including support for
    Ray-based asynchronous data resolution.

    Parameters
    ----------
    products : list of Product, optional
        Initial list of products. Defaults to an empty list.
    """

    def __init__(self, products=None):
        if products is None:
            products = []
        self._products = products

    @property
    def names(self):
        """List of product names."""
        return [p.name for p in self._products]

    @property
    def products_unique_names(self):
        """List of unique product names. Return formatted as step_name.product_name.product_id."""
        names = []
        for p in self._products:
            components = [p.step_name, p.name, p.id]
            name = ".".join(components)
            names.append(name)
        return names

    @property
    def data(self):
        """List of product data."""
        return [p.data for p in self._products]

    def index(self, key):
        """
        Get the index of a product by name or position.

        Parameters
        ----------
        key : int, slice, or str
            Index or name of the product.

        Returns
        -------
        int or slice
            Index corresponding to the key.
        """
        if isinstance(key, (int, slice)):
            return key
        elif isinstance(key, str):
            return self.names.index(key)

    def copy(self):
        """Create a deep copy of the products container."""
        return self.__class__([p.copy() for p in self])

    def __len__(self):
        return len(self._products)

    def __getitem__(self, key):
        if isinstance(key, np.ndarray) and key.dtype == bool:
            return self.__class__(list(np.array(self._products)[key]))
        else:
            return self._products[self.index(key)]

    def __delitem__(self, key):
        del self._products[self.index(key)]

    def insert(self, idx, model):
        self._products.insert(idx, model)

    def __setitem__(self, key, product):
        if isinstance(product, Product):
            if isinstance(key, np.ndarray) and key.dtype == bool:
                ind = np.where(key)[0]
                for idx in ind:
                    self._products[int(idx)] = product
            else:
                self._products[self.index(key)] = product
        else:
            raise TypeError(f"Invalid type: {product!r}")

    def select_from_list(self, required_products):
        """
        Select products based on a list of criteria.

        Parameters
        ----------
        required_products : list of dict
            List of selection criteria dictionaries passed to select method.

        Returns
        -------
        Products
            A new `Products` instance with selected products.
        """
        selection = Products([])
        for req in required_products:
            selection.extend(self.select(**req))
        return selection

    def select(self, name=None, step_name=None, pid=None):
        """
        Select products matching the given criteria.

        Parameters
        ----------
        tag : str, optional
            Name to match.
        step_name : str, optional
            Step name to match.
        pid : str, optional
            Product pid to match.

        Returns
        -------
        Products
            A new `Products` instance with matching products.
        """

        mask = self.selection_mask(name=name, step_name=step_name, pid=pid)
        return self[mask]

    def selection_mask(self, name=None, step_name=None, pid=None):
        """
        Create a boolean mask for selecting products.

        Parameters
        ----------
        name : str, optional
            Names to match.
        step_name : str, optional
            Step names to match.
        pid : str, optional
            Product pid to match.

        Returns
        -------
        mask : np.ndarray
            Boolean array indicating selected products.
        """

        mask = np.ones(len(self), dtype=bool)
        for idx, p in enumerate(self._products):
            if name is not None:
                mask[idx] &= p.name == name
            if step_name is not None:
                mask[idx] &= p.step_name == step_name
            if name is not None:
                mask[idx] &= p.pid == pid
        return mask

    @requires_module("ray")
    def get(self):
        """
        Resolve and update products whose data is a Ray ObjectRef.
        Only available if `ray` is available.

        This method fetches remote data using `ray.get()` and updates
        the corresponding products in-place.

        """
        ind = np.where([p.needs_update for p in self._products])[0]
        refs_to_run = {data for data in np.array(self.data)[ind]}  # unique objects
        if refs_to_run:
            results = ray.get(list(refs_to_run))
            products = [item for sublist in results for item in sublist]  # flatten
            for product in products:
                mask = self.selection_mask(name=product.name)
                self[mask] = product


class WorkflowStepBase(abc.ABC):
    """A step of the analysis workflow.

    Parameters
    ----------
    config : dict
        Configuration dictionary for the step.
    log : object, optional
        Logger instance (not used in this implementation).
    name : str, optional
        Name identifier for the step. If not provided, a unique name is generated.
    overwrite : bool, optional
        Whether to overwrite existing outputs. Default is True.
    """

    tag = "workflow-step"
    required_data = []
    products_names = []
    parallel = False
    _products_as_workfllow_attributes = False

    def __init__(self, config, log=None, name=None, overwrite=True):
        self.config = config
        self.overwrite = overwrite
        self.name = make_name(name)
        self._data = Products()
        self.products = Products(
            [Product(name=_, step_name=name) for _ in self.products_names]
        )
        if log is None:
            log = logging.getLogger(__name__)
        self.log = log

    @property
    def data(self):
        """Input data for the workflow step."""
        return self._data

    def set_data(self, data, extend=True):
        """
        Set the input data for the workflow step.

        Parameters
        ----------
        data : Product, list, or Products
            The input data to be used by the step.
        extend : bool, optional
            If False, replace the existing data.
            If True, extend the existing data.
            Default is True.
        """
        if isinstance(data, Product):
            data = Products([data])
        elif isinstance(data, list):
            data = Products(data)
        else:
            self._data = data
            return None

        selection = data.select_from_list(self.required_data)
        if extend:
            self._data.extend(selection)
        else:
            self._data = selection

    def run(self, data, extend=True, parallel=None):
        """
        Execute the workflow step.

        Parameters
        ----------
        data : Product, list, or Products
            Input data to be used for the step.
        extend : bool, optional
            Whether to extend or replace the current data. Default is True.
        parallel : bool, optional
            Whether to run the step in parallel using Ray.
            Defalut is None and the class default is used.

        Returns
        -------
        products : Products
            The output products generated by the step.
        """
        self.set_data(
            data, extend=extend
        )  # copied or reference if data is on the object store ?
        if not self.overwrite:
            self.products = self.products.copy()  # generate new id at each run
        if parallel is None:
            parallel = self.parallel
        if parallel and ray is not None:
            run_parrallel = ray.remote(self.__class__._run)
            ref = run_parrallel.remote(self)
            for p in self.products:
                p.data = ref
        else:
            self._run()

        if self._products_as_workfllow_attributes:
            for product_tag in self.products_names:
                setattr(self.data, product_tag, self.products[product_tag].data)
        return self.products

    @abc.abstractmethod
    def _run(self):
        self.data.get()  # wait other remote and set results on self.data
        pass


class WorkflowStep:
    """Create one of the workflow step class listed in the registry."""

    @staticmethod
    def create(tag, config, **kwargs):
        """
        Create a workflow step instance from the registry.

        Parameters
        ----------
        tag : str
            Identifier for the workflow step.
        config : dict or `~gammapy.analysis.WorkflowConfig`
            Configuration options following `WorkflowConfig` schema.
        **kwargs : dict, optional
            Additional keyword arguments passed to the workflow step constructor.

        """

        from . import WORKFLOW_STEP_REGISTRY

        cls = WORKFLOW_STEP_REGISTRY.get_cls(tag)
        return cls(config, **kwargs)


class DataSelectionWorkflowStep(WorkflowStepBase):
    tag = "data-selection"

    def _run(self):
        filepath = self.config.general.datasets_file
        if filepath is not None and os.path.exists(filepath) and not self.overwrite:
            self.data.read_datasets()
        else:
            obs_step = ObservationsWorkflowStep(
                self.config, log=self.log, overwrite=self.overwrite
            )
            datasets_step = DatasetsWorkflowStep(
                self.config, log=self.log, overwrite=self.overwrite
            )
            obs_step.run(self.data)
            datasets_step.run(self.data)
            self.data.write_datasets()


class ObservationsWorkflowStep(WorkflowStepBase):
    tag = "observations"

    def _run(self):
        """Fetch observations from the data store according to criteria defined in the configuration."""
        observations_settings = self.config.observations
        self.data.datastore = self._get_data_store()

        self.log.info("Fetching observations.")
        ids = self._make_obs_table_selection()
        required_irf = observations_settings.required_irf
        self.data.observations = self.data.datastore.get_observations(
            ids, skip_missing=True, required_irf=required_irf
        )

        if observations_settings.obs_time.start is not None:
            start = observations_settings.obs_time.start
            stop = observations_settings.obs_time.stop
            if len(start.shape) == 0:
                time_intervals = [(start, stop)]
            else:
                time_intervals = [(tstart, tstop) for tstart, tstop in zip(start, stop)]
            self.data.observations = self.data.observations.select_time(time_intervals)

        self.log.info(f"Number of selected observations: {len(self.data.observations)}")

        for obs in self.data.observations:
            self.log.debug(obs)

    def _get_data_store(self):
        """Set the datastore on the Workflow object."""
        path = make_path(self.config.observations.datastore)
        if path.is_file():
            self.log.debug(f"Setting datastore from file: {path}")
            return DataStore.from_file(path)
        elif path.is_dir():
            self.log.debug(f"Setting datastore from directory: {path}")
            return DataStore.from_dir(path)
        else:
            raise FileNotFoundError(f"Datastore not found: {path}")

    def _make_obs_table_selection(self):
        """Return list of obs_ids after filtering on datastore observation table."""
        obs_settings = self.config.observations

        # Reject configs with list of obs_ids and obs_file set at the same time
        if len(obs_settings.obs_ids) and obs_settings.obs_file is not None:
            raise ValueError(
                "Values for both parameters obs_ids and obs_file are not accepted."
            )

        # First select input list of observations from obs_table
        if len(obs_settings.obs_ids):
            selected_obs_table = self.data.datastore.obs_table.select_obs_id(
                obs_settings.obs_ids
            )
        elif obs_settings.obs_file is not None:
            path = make_path(obs_settings.obs_file)
            ids = list(Table.read(path, format="ascii", data_start=0).columns[0])
            selected_obs_table = self.data.datastore.obs_table.select_obs_id(ids)
        else:
            selected_obs_table = self.data.datastore.obs_table

        # Apply cone selection
        if obs_settings.obs_cone.lon is not None:
            cone = dict(
                type="sky_circle",
                frame=obs_settings.obs_cone.frame,
                lon=obs_settings.obs_cone.lon,
                lat=obs_settings.obs_cone.lat,
                radius=obs_settings.obs_cone.radius,
                border="0 deg",
            )
            selected_obs_table = selected_obs_table.select_observations(cone)

        return selected_obs_table["OBS_ID"].tolist()


class DatasetsWorkflowStep(WorkflowStepBase):
    tag = "datasets"
    products_names = ["datasets"]
    _products_as_workfllow_attributes = True

    def _run(self):
        # TODO: check if exits and read else run and write
        self.get_datasets()

    def get_datasets(self):
        """Produce reduced datasets."""
        datasets_settings = self.config.datasets
        if not self.data.observations or len(self.data.observations) == 0:
            raise RuntimeError("No observations have been selected.")

        if datasets_settings.type == "1d":
            self._spectrum_extraction()
        else:  # 3d
            self._map_making()

    @staticmethod
    def _create_wcs_geometry(wcs_geom_settings, axes):
        """Create the WCS geometry."""
        geom_params = {}
        skydir_settings = wcs_geom_settings.skydir
        if skydir_settings.lon is not None:
            skydir = SkyCoord(
                skydir_settings.lon, skydir_settings.lat, frame=skydir_settings.frame
            )
            geom_params["skydir"] = skydir

        if skydir_settings.frame in ["icrs", "galactic"]:
            geom_params["frame"] = skydir_settings.frame
        else:
            raise ValueError(
                f"Incorrect skydir frame: expect 'icrs' or 'galactic'. Got {skydir_settings.frame}"
            )

        geom_params["axes"] = axes
        geom_params["binsz"] = wcs_geom_settings.binsize
        width = wcs_geom_settings.width.width.to("deg").value
        height = wcs_geom_settings.width.height.to("deg").value
        geom_params["width"] = (width, height)

        return WcsGeom.create(**geom_params)

    @staticmethod
    def _create_region_geometry(on_region_settings, axes):
        """Create the region geometry."""
        on_lon = on_region_settings.lon
        on_lat = on_region_settings.lat
        on_center = SkyCoord(on_lon, on_lat, frame=on_region_settings.frame)
        on_region = CircleSkyRegion(on_center, on_region_settings.radius)

        return RegionGeom.create(region=on_region, axes=axes)

    def _create_geometry(self):
        """Create the geometry."""
        self.log.debug("Creating geometry.")
        datasets_settings = self.config.datasets
        geom_settings = datasets_settings.geom
        axes = [make_energy_axis(geom_settings.axes.energy)]
        if datasets_settings.type == "3d":
            geom = self._create_wcs_geometry(geom_settings.wcs, axes)
        elif datasets_settings.type == "1d":
            geom = self._create_region_geometry(datasets_settings.on_region, axes)
        else:
            raise ValueError(
                f"Incorrect dataset type. Expect '1d' or '3d'. Got {datasets_settings.type}."
            )
        return geom

    def _create_reference_dataset(self, name=None):
        """Create the reference dataset for the current workflow."""
        self.log.debug("Creating target Dataset.")
        geom = self._create_geometry()

        geom_settings = self.config.datasets.geom
        geom_irf = dict(energy_axis_true=None, binsz_irf=None)
        if geom_settings.axes.energy_true.min is not None:
            geom_irf["energy_axis_true"] = make_energy_axis(
                geom_settings.axes.energy_true, name="energy_true"
            )
        if geom_settings.wcs.binsize_irf is not None:
            geom_irf["binsz_irf"] = geom_settings.wcs.binsize_irf.to("deg").value

        if self.config.datasets.type == "1d":
            return SpectrumDataset.create(geom, name=name, **geom_irf)
        else:
            return MapDataset.create(geom, name=name, **geom_irf)

    def _create_dataset_maker(self):
        """Create the Dataset Maker."""
        self.log.debug("Creating the target Dataset Maker.")

        datasets_settings = self.config.datasets
        if datasets_settings.type == "3d":
            maker = MapDatasetMaker(selection=datasets_settings.map_selection)
        elif datasets_settings.type == "1d":
            maker_config = {}
            if datasets_settings.containment_correction:
                maker_config["containment_correction"] = (
                    datasets_settings.containment_correction
                )

            maker_config["selection"] = ["counts", "exposure", "edisp"]

            maker = SpectrumDatasetMaker(**maker_config)

        return maker

    def _create_safe_mask_maker(self):
        """Create the SafeMaskMaker."""
        self.log.debug("Creating the mask_safe Maker.")

        safe_mask_selection = self.config.datasets.safe_mask.methods
        safe_mask_settings = self.config.datasets.safe_mask.parameters
        return SafeMaskMaker(methods=safe_mask_selection, **safe_mask_settings)

    def _create_background_maker(self):
        """Create the Background maker."""
        self.log.info("Creating the background Maker.")

        datasets_settings = self.config.datasets
        bkg_maker_config = {}
        if datasets_settings.background.exclusion:
            path = make_path(datasets_settings.background.exclusion)
            exclusion_mask = Map.read(path)
            exclusion_mask.data = exclusion_mask.data.astype(bool)
            bkg_maker_config["exclusion_mask"] = exclusion_mask
        bkg_maker_config.update(datasets_settings.background.parameters)

        bkg_method = datasets_settings.background.method

        bkg_maker = None
        if bkg_method == "fov_background":
            self.log.debug(
                f"Creating FoVBackgroundMaker with arguments {bkg_maker_config}"
            )
            bkg_maker = FoVBackgroundMaker(**bkg_maker_config)
        elif bkg_method == "ring":
            bkg_maker = RingBackgroundMaker(**bkg_maker_config)
            self.log.debug(
                f"Creating RingBackgroundMaker with arguments {bkg_maker_config}"
            )
            if datasets_settings.geom.axes.energy.nbins > 1:
                raise ValueError(
                    "You need to define a single-bin energy geometry for your dataset."
                )
        elif bkg_method == "reflected":
            bkg_maker = ReflectedRegionsBackgroundMaker(**bkg_maker_config)
            self.log.debug(
                f"Creating ReflectedRegionsBackgroundMaker with arguments {bkg_maker_config}"
            )
        else:
            self.log.warning("No background maker set. Check configuration.")
        return bkg_maker

    def _map_making(self):
        """Make maps and datasets for 3d workflow"""
        datasets_settings = self.config.datasets
        offset_max = datasets_settings.geom.selection.offset_max

        self.log.info("Creating reference dataset and makers.")
        stacked = self._create_reference_dataset(name="stacked")

        maker = self._create_dataset_maker()
        maker_safe_mask = self._create_safe_mask_maker()
        bkg_maker = self._create_background_maker()

        makers = [maker, maker_safe_mask, bkg_maker]
        makers = [maker for maker in makers if maker is not None]

        self.log.info("Start the data reduction loop.")

        datasets_maker = DatasetsMaker(
            makers,
            stack_datasets=datasets_settings.stack,
            n_jobs=self.config.general.n_jobs,
            cutout_mode="trim",
            cutout_width=2 * offset_max,
        )
        datasets = datasets_maker.run(stacked, self.data.observations)
        self.products["datasets"].data = Datasets(datasets)

    def _spectrum_extraction(self):
        """Run all steps for the spectrum extraction."""
        self.log.info("Reducing spectrum datasets.")
        datasets_settings = self.config.datasets
        dataset_maker = self._create_dataset_maker()
        safe_mask_maker = self._create_safe_mask_maker()
        bkg_maker = self._create_background_maker()

        reference = self._create_reference_dataset()

        datasets = []
        for obs in progress_bar(self.data.observations, desc="Observations"):
            self.log.debug(f"Processing observation {obs.obs_id}")
            dataset = dataset_maker.run(reference.copy(), obs)
            if bkg_maker is not None:
                dataset = bkg_maker.run(dataset, obs)
                if dataset.counts_off is None:
                    self.log.debug(
                        f"No OFF region found for observation {obs.obs_id}. Discarding."
                    )
                    continue
            dataset = safe_mask_maker.run(dataset, obs)
            self.log.debug(dataset)
            datasets.append(dataset)

        if datasets_settings.stack:
            datasets = Datasets(datasets).stack_reduce(name="stacked")

        self.products["datasets"].data = Datasets(datasets)


def make_energy_axis(axis, name="energy"):
    if axis.min is None or axis.max is None:
        return None
    elif axis.nbins is None or axis.nbins < 1:
        return None
    else:
        return MapAxis.from_bounds(
            name=name,
            lo_bnd=axis.min.value,
            hi_bnd=axis.max.to_value(axis.min.unit),
            nbin=axis.nbins,
            unit=axis.min.unit,
            interp="log",
            node_type="edges",
        )


class ExcessMapWorkflowStep(WorkflowStepBase):
    tag = "excess-map"
    products_names = ["excess_map"]
    _products_as_workfllow_attributes = True

    def _run(self):
        """Calculate excess map with respect to the current model."""
        excess_settings = self.config.excess_map
        # TODO: allow a list of kernel sizes
        self.log.info("Computing excess maps.")

        if self.config.datasets.type == "1d":
            raise ValueError("Cannot compute excess map for 1D dataset")

        # Here we could possibly stack the datasets if needed.
        if len(self.data.datasets) > 1:
            raise ValueError("Datasets must be stacked to compute the excess map")

        energy_edges = make_energy_axis(excess_settings.energy_edges)
        if energy_edges is not None:
            energy_edges = energy_edges.edges

        excess_map_estimator = ExcessMapEstimator(
            correlation_radius=excess_settings.correlation_radius,
            energy_edges=energy_edges,
            **excess_settings.parameters,
        )
        self.products["excess_map"].data = excess_map_estimator.run(
            self.data.datasets[0]
        )


class FitWorkflowStep(WorkflowStepBase):
    tag = "fit"
    products_names = ["fit_result"]
    _products_as_workfllow_attributes = True

    def _run(self):
        """Fitting reduced datasets to model."""
        if not self.data.models:
            raise RuntimeError("Missing models")

        fit_settings = self.config.fit
        for dataset in self.data.datasets:
            if fit_settings.fit_range:
                energy_min = fit_settings.fit_range.min
                energy_max = fit_settings.fit_range.max
                # TODO : add fit range in lon/lat
                geom = dataset.counts.geom
                dataset.mask_fit = geom.energy_mask(energy_min, energy_max)

        self.log.info("Fitting datasets.")
        result = self.data.fit.run(datasets=self.data.datasets)
        self.products["fit_result"].data = result
        self.log.info(result)


class FluxPointsWorkflowStep(WorkflowStepBase):
    tag = "flux-points"
    products_names = ["flux_points"]
    _products_as_workfllow_attributes = True

    def _run(self):
        """Calculate flux points for a specific model component."""
        if not self.data.datasets:
            raise RuntimeError(
                "No datasets defined. Impossible to compute flux points."
            )

        fp_settings = self.config.flux_points
        self.log.info("Calculating flux points.")

        energy_edges = make_energy_axis(fp_settings.energy).edges
        flux_point_estimator = FluxPointsEstimator(
            energy_edges=energy_edges,
            source=fp_settings.source,
            fit=self.data.fit,
            **fp_settings.parameters,
        )

        fp = flux_point_estimator.run(datasets=self.data.datasets)

        flux_points = FluxPointsDataset(
            data=fp, models=self.data.models[fp_settings.source]
        )
        cols = ["e_ref", "dnde", "dnde_ul", "dnde_err", "sqrt_ts"]
        table = flux_points.data.to_table(sed_type="dnde")
        self.log.info("\n{}".format(table[cols]))
        self.products["flux_points"].data = flux_points


class LightCurveWorkflowStep(WorkflowStepBase):
    tag = "light-curve"
    products_names = ["light_curve"]
    _products_as_workfllow_attributes = True

    def _run(self):
        """Calculate light curve for a specific model component."""
        lc_settings = self.config.light_curve
        self.log.info("Computing light curve.")
        energy_edges = make_energy_axis(lc_settings.energy_edges).edges

        if (
            lc_settings.time_intervals.start is None
            or lc_settings.time_intervals.stop is None
        ):
            self.log.info(
                "Time intervals not defined. Extract light curve on datasets GTIs."
            )
            time_intervals = None
        else:
            time_intervals = [
                (t1, t2)
                for t1, t2 in zip(
                    lc_settings.time_intervals.start, lc_settings.time_intervals.stop
                )
            ]

        light_curve_estimator = LightCurveEstimator(
            time_intervals=time_intervals,
            energy_edges=energy_edges,
            source=lc_settings.source,
            fit=self.data.fit,
            **lc_settings.parameters,
        )
        lc = light_curve_estimator.run(datasets=self.data.datasets)
        self.products["light_curve"].data = lc
        self.log.info("\n{}".format(lc.to_table(format="lightcurve", sed_type="flux")))
